{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699e9df8",
   "metadata": {},
   "source": [
    "# Test multinode multigpu training with deepspeed and ray",
    "",
    "This notebook requires access to a running Ray cluster with GPUs available on each node. Make sure the cluster is started and that the notebook kernel is connected to it before executing any code.",
    "",
    "Execute the cells sequentially to launch the training job using Ray and DeepSpeed.",
    "",
    "Key configuration files:",
    "- `config/train_config.yaml` \u2013 overall training parameters.",
    "- `config/ds_config.json` \u2013 DeepSpeed ZeRO strategy settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09277710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DeepSpeedStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from ray.train.lightning import RayLightningTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "\n",
    "# LightningModule for training your Hugging Face LLM\n",
    "class LLMModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, learning_rate=1e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        return outputs.loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self(**batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "\n",
    "# Dataset preparation\n",
    "def prepare_dataset(tokenizer, seq_length=512):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=seq_length\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    return DataLoader(tokenized_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "# Training function (Ray calls this internally)\n",
    "def train_func(config):\n",
    "    model_name = \"gpt2\"  # replace with your desired LLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    train_loader = prepare_dataset(tokenizer)\n",
    "\n",
    "    model = LLMModel(model_name=model_name)\n",
    "\n",
    "    # DeepSpeed ZeRO-3 strategy with CPU offload\n",
    "    strategy = DeepSpeedStrategy(config=\"ds_config.json\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        strategy=strategy,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=torch.cuda.device_count(),\n",
    "        precision=16,\n",
    "        max_epochs=3\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader)\n",
    "\n",
    "\n",
    "# Main entry-point (Ray handles the distributed training orchestration)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    scaling_config = ScalingConfig(\n",
    "        num_workers=8,  # total number of GPUs across your nodes\n",
    "        use_gpu=True,\n",
    "        resources_per_worker={\"CPU\": 8, \"GPU\": 1},\n",
    "    )\n",
    "\n",
    "    trainer = RayLightningTrainer(\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=None,\n",
    "        lightning_config={},\n",
    "        trainer_init_config={},\n",
    "    )\n",
    "\n",
    "    trainer.fit(train_func)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
