# =========================================================================
# Unified Training Configuration
# =========================================================================
# This file is the single source of truth for a training run.
# It uses the "inline DeepSpeed config" approach for maximum clarity
# and stability, preventing conflicts between PyTorch Lightning and DeepSpeed.
# -------------------------------------------------------------------------

run:
  seed: 42
  project: "llm-multinode-training" # Wandb project name
  run_name: "gpt2-wikitext-oscar-test" # Wandb run name (can be dynamic)
  use_wandb: true
  wandb_log_model: false # Logging model artifacts can be slow and use a lot of space.
  lr_monitor_logging_interval: "step" # Log learning rate every step.

  # --- Ray Train Checkpointing ---
  # These are handled by Ray Train's RunConfig, not PyTorch Lightning's callbacks.
  ray_checkpoint_monitor: "val_loss" # Metric Ray Train watches to save the best checkpoint.
  ray_checkpoint_mode: "min"         # "min" for loss/perplexity, "max" for accuracy.
  ray_checkpoint_num_to_keep: 2      # Number of best checkpoints to keep.

model:
  # --- Specify which model class to use ---
  module_path: "src.models.default_models"
  class_name: "HuggingFaceLLM"

  # --- Keyword arguments passed to the model's __init__ ---
  kwargs:
    # --- Core Model & Tokenizer Specification ---
    model_name_or_path: "gpt2"
    model_cache_dir: "data/model_cache"
    tokenizer_cache_dir: "data/model_cache"

    # --- Optimizer & Scheduler Hyperparameters ---
    # NOTE: All optimizer and scheduler settings have been moved to the
    # 'strategy.config_dict' section below. This allows DeepSpeed to manage
    # them natively, which is more robust and performant. Your LightningModule's
    # 'configure_optimizers' method should now simply return None.

    # --- Generation Hyperparameters (for predict_step) ---
    predict_max_length: 128
    predict_num_beams: 1
    DEBUG_MODE: False

datamodule:
  kwargs:
    dataset_configs:
      wikitext: {}
      oscar:
        train_split_percentage: [0.001, 0.002]
    
    # This MUST match 'train_micro_batch_size_per_gpu' in the DeepSpeed config below.
    batch_size: 4
    
    max_length: 128
    num_workers: 25
    cache_dir: "data/main_cache"

trainer:
  # Runs one training batch and one validation batch then exits.
  fast_dev_run: false
  # Track the L2 norm of the gradients. It will automatically appear in W&B.
  track_grad_norm: 2

  # --- Arguments for the PyTorch Lightning Trainer ---
  max_epochs: 3
  max_steps: -1 # If > 0, this overrides max_epochs.
  accelerator: "gpu"
  devices: "auto" # Let PL/Ray handle it. Will be 1 per worker.
  
  # 'bf16-mixed' is generally preferred on modern GPUs (A100/H100).
  # '16-mixed' is for older GPUs.
  precision: "16-mixed"
  # NOTE: 'gradient_clip_val' is now handled by 'gradient_clipping' in the DeepSpeed config.
  # gradient_clip_val: 1.0
  val_check_interval: 200
  log_every_n_steps: 20

  # This tells Lightning how many batches to process before performing an optimizer step.
  # This value is used by DeepSpeed to calculate the effective global batch size.
  # Global Batch Size = (train_micro_batch_size_per_gpu * num_workers * accumulate_grad_batches)
  accumulate_grad_batches: 2

strategy:
  # --- Configuration for the Lightning Strategy ---
  name: "deepspeed"
  find_unused_parameters: false

  # --- INLINE DEEPSPEED CONFIG ---
  # By defining the config here, we avoid a separate .json file and can add comments.
  # Your train.py script is already set up to read from 'config_dict' if it exists.
  config_dict:
    # --- Batch Size and Precision ---
    # This is the actual batch size processed by one GPU in a single forward/backward pass.
    # It must match 'datamodule.kwargs.batch_size' for correct data loading.
    train_micro_batch_size_per_gpu: 4
    fp16:
      enabled: true
      
    # --- Gradient Clipping and Accumulation ---
    # NOTE: 'gradient_accumulation_steps' is NOT set here.
    # Lightning controls this via 'trainer.accumulate_grad_batches'.
    gradient_clipping: 1.0

    # --- Optimizer Configuration ---
    # DeepSpeed will create and manage the optimizer internally.
    optimizer:
      type: "AdamW" # The recommended optimizer for transformers.
      params:
        lr: 5.0e-5
        betas: [0.9, 0.999]
        eps: 1.0e-8
        weight_decay: 0.01

    # --- Scheduler Configuration ---
    # DeepSpeed also manages the learning rate scheduler.
    scheduler:
      type: "WarmupLR" # A standard linear warmup and decay schedule.
      params:
        warmup_min_lr: 0
        warmup_max_lr: 5.0e-5 # Should match the peak LR in the optimizer.
        # This needs to be calculated: (warmup_ratio * total_training_steps).
        # Example: 6% warmup for 100k steps = 6000.
        warmup_num_steps: 6000

    # --- ZeRO (Zero Redundancy Optimizer) Configuration ---
    zero_optimization:
      # Stage 3 is the most memory-efficient: it partitions model weights,
      # gradients, AND optimizer states across all GPUs. Essential for huge models.
      stage: 3

      # Offload optimizer state and parameters to regular CPU RAM.
      # This is critical for fitting massive models that don't fit in VRAM.
      # DeepSpeed will automatically use a CPU-Adam optimizer when this is enabled.
      offload_optimizer:
        device: "none"
        pin_memory: true
      offload_param:
        device: "none" #none, cpu, nvme
        pin_memory: true

      # --- Advanced Performance Tuning for Stage 3 ---
      overlap_comm: true            # Overlap communication with computation.
      contiguous_gradients: true    # Reduces memory fragmentation.
      reduce_bucket_size: 500_000_000        # YAML IS RETARDED DOESN't understand 5e8 .Size of buckets for gradient reduction.
      stage3_prefetch_bucket_size: 500_000_000  # Size of buckets for parameter prefetching.
      stage3_param_persistence_threshold: 1_000_000   # Keep small params on GPU.

callbacks:
  generation_logger:
    # Set to false to completely disable this callback for fast runs.
    # Set to true to enable it for qualitative evaluation.
    enabled: false 
    # Choose from 'default', 'creative', 'qa', 'code'
    prompt_set_name: "default" 
    # Set to a positive integer to log during validation, e.g., every 50 batches.
    # Set to -1 or 0 to only log at the end of the entire validation run.
    log_every_n_steps: -1

ray:
  # --- Configuration for Ray Train ---
  use_ray: true
  address: "auto"
  num_workers: 2      # Total number of GPUs you want to use for training.
  cpus_per_worker: 8  # CPUs for data loading and other tasks on each worker.
  storage_path: "/tmp/ray_train_results"