{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0109a1",
   "metadata": {},
   "source": [
    "# Test multinode multigpu training with deepspeed and ray\n",
    "\n",
    "Tokenized datasets are cached under `data/main_cache`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2445ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIAO FROM CYPHER, BIATCH\n"
     ]
    }
   ],
   "source": [
    "print(\"CIAO FROM CYPHER, BIATCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be1cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 17:56:46,292\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 10.141.1.24:6379...\n",
      "[2025-07-02 17:56:51,315 W 3415416 3415416] gcs_rpc_client.h:151: Failed to connect to GCS at address 10.141.1.24:6379 within 5 seconds.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.init(address=\"10.141.1.24:6379\")\n",
    "\n",
    "if ray.is_initialized():\n",
    "    print(\"RAy already up and running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a56ee0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object_store_memory': 318480340991.0,\n",
       " 'accelerator_type:A100': 2.0,\n",
       " 'node:__internal_head__': 1.0,\n",
       " 'memory': 743120795649.0,\n",
       " 'GPU': 8.0,\n",
       " 'CPU': 96.0,\n",
       " 'node:10.141.1.24': 1.0,\n",
       " 'node:10.141.1.44': 1.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b15fe80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-26 18:21:42,475] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 18:21:42,789 - INFO - gcc -pthread -B /davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /davinci-1/home/abeatini/.conda/envs/shallow/include -fPIC -O2 -isystem /davinci-1/home/abeatini/.conda/envs/shallow/include -fPIC -c /tmp/tmp4pe4kps9/test.c -o /tmp/tmp4pe4kps9/test.o\n",
      "2025-06-26 18:21:42,810 - INFO - gcc -pthread -B /davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat /tmp/tmp4pe4kps9/test.o -laio -o /tmp/tmp4pe4kps9/a.out\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "2025-06-26 18:21:43,418 - INFO - gcc -pthread -B /davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /davinci-1/home/abeatini/.conda/envs/shallow/include -fPIC -O2 -isystem /davinci-1/home/abeatini/.conda/envs/shallow/include -fPIC -c /tmp/tmp14lllmos/test.c -o /tmp/tmp14lllmos/test.o\n",
      "2025-06-26 18:21:43,439 - INFO - gcc -pthread -B /davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat /tmp/tmp14lllmos/test.o -L/cm/shared/apps/cuda12.1/toolkit/12.1.1 -L/cm/shared/apps/cuda12.1/toolkit/12.1.1/lib64 -lcufile -o /tmp/tmp14lllmos/a.out\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: warning: libm.so.6, needed by /cm/shared/apps/cuda12.1/toolkit/12.1.1/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: /cm/shared/apps/cuda12.1/toolkit/12.1.1/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: /cm/local/apps/gcc/11.2.0/lib64/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: /cm/shared/apps/cuda12.1/toolkit/12.1.1/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: /cm/local/apps/gcc/11.2.0/lib64/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: /cm/shared/apps/cuda12.1/toolkit/12.1.1/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: /cm/shared/apps/cuda12.1/toolkit/12.1.1/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/davinci-1/home/abeatini/.conda/envs/shallow/compiler_compat/ld: /cm/shared/apps/cuda12.1/toolkit/12.1.1/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# Ray-aware DeepSpeed ZeRO-3 strategy\n",
    "import json\n",
    "from pathlib import Path\n",
    "config_path = Path(\"/davinci-1/home/abeatini/pycharmProjects/shallowMind/config/ds_config.json\")\n",
    "\n",
    "# Read the file’s contents and turn it into a Python dict\n",
    "with config_path.open() as f:\n",
    "    ds_config = json.load(f)          # <-- use json.load, not json.loads\n",
    "strategy = RayDeepSpeedStrategy(ds_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d59cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cfg = {\n",
    "    \"train_micro_batch_size_per_gpu\": 4,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"fp16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "        \"offload_param\":    {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"reduce_bucket_size\":            500_000_000,\n",
    "        \"stage3_prefetch_bucket_size\":   500_000_000,\n",
    "        \"stage3_param_persistence_threshold\": 1_000_000\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Ray-aware DeepSpeed ZeRO-3 strategy\n",
    "strategy = RayDeepSpeedStrategy(stage=3, config=ds_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:01:27,363\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAy already up and running\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:01:27 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:01:32 (running for 00:00:05.12)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:01:37 (running for 00:00:10.14)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:01:42 (running for 00:00:15.19)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:01:47 (running for 00:00:20.20)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:01:52 (running for 00:00:25.22)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:01:57 (running for 00:00:30.24)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:02 (running for 00:00:35.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:07 (running for 00:00:40.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:12 (running for 00:00:45.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:17 (running for 00:00:50.29)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:22 (running for 00:00:55.31)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:27 (running for 00:01:00.33)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:32 (running for 00:01:05.34)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:37 (running for 00:01:10.36)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:42 (running for 00:01:15.37)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:47 (running for 00:01:20.39)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:52 (running for 00:01:25.40)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:02:57 (running for 00:01:30.42)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:02 (running for 00:01:35.43)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:07 (running for 00:01:40.45)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:12 (running for 00:01:45.46)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:17 (running for 00:01:50.48)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:22 (running for 00:01:55.49)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:27 (running for 00:02:00.51)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:32 (running for 00:02:05.53)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:37 (running for 00:02:10.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:42 (running for 00:02:15.58)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:47 (running for 00:02:20.60)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:52 (running for 00:02:25.61)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:03:58 (running for 00:02:30.63)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:03 (running for 00:02:35.65)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:08 (running for 00:02:40.67)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:13 (running for 00:02:45.68)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:18 (running for 00:02:50.70)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:23 (running for 00:02:55.72)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:28 (running for 00:03:00.74)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:33 (running for 00:03:05.76)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:38 (running for 00:03:10.78)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:43 (running for 00:03:15.79)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:04:48,292\tERROR checkpoint_manager.py:144 -- Result dict has no key: val_loss. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['train_loss', 'epoch', 'step', 'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'training_iteration', 'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n",
      "2025-06-26 19:04:48,292\tERROR checkpoint_manager.py:144 -- Result dict has no key: val_loss. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['train_loss', 'epoch', 'step', 'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'training_iteration', 'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:48 (running for 00:03:20.81)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:53 (running for 00:03:25.84)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:04:58 (running for 00:03:30.85)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:03 (running for 00:03:35.87)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:08 (running for 00:03:40.88)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:13 (running for 00:03:45.90)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:18 (running for 00:03:50.91)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:23 (running for 00:03:55.92)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:28 (running for 00:04:00.94)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:33 (running for 00:04:05.95)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:38 (running for 00:04:10.97)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:43 (running for 00:04:15.98)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:48 (running for 00:04:20.99)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:53 (running for 00:04:26.00)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:05:58,478\tERROR checkpoint_manager.py:144 -- Result dict has no key: val_loss. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['train_loss', 'epoch', 'step', 'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'training_iteration', 'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n",
      "2025-06-26 19:05:58,479\tERROR checkpoint_manager.py:144 -- Result dict has no key: val_loss. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['train_loss', 'epoch', 'step', 'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'training_iteration', 'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n",
      "2025-06-26 19:05:58,480\tERROR checkpoint_manager.py:144 -- Result dict has no key: val_loss. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['train_loss', 'epoch', 'step', 'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'training_iteration', 'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n",
      "2025-06-26 19:05:58,480\tERROR checkpoint_manager.py:144 -- Result dict has no key: val_loss. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['train_loss', 'epoch', 'step', 'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'training_iteration', 'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-26 19:05:58 (running for 00:04:31.02)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:06:01,186\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/davinci-1/home/abeatini/ray_results/TorchTrainer_2025-06-26_19-01-27' in 0.0045s.\n",
      "2025-06-26 19:06:01,189\tINFO tune.py:1041 -- Total run time: 273.83 seconds (273.81 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-26 19:06:01 (running for 00:04:33.82)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 17.0/96 CPUs, 8.0/8 GPUs (0.0/2.0 accelerator_type:A100)\n",
      "Result logdir: /var/tmp/pbs.1874768.davinci-mgt01/ray/session_2025-06-26_17-23-15_385852_3806738/artifacts/2025-06-26_19-01-27/TorchTrainer_2025-06-26_19-01-27/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "Result(\n",
      "  metrics={'train_loss': 2.669545888900757, 'epoch': 2, 'step': 888},\n",
      "  path='/davinci-1/home/abeatini/ray_results/TorchTrainer_2025-06-26_19-01-27/TorchTrainer_32cee_00000_0_2025-06-26_19-01-27',\n",
      "  filesystem='local',\n",
      "  checkpoint=Checkpoint(filesystem=local, path=/davinci-1/home/abeatini/ray_results/TorchTrainer_2025-06-26_19-01-27/TorchTrainer_32cee_00000_0_2025-06-26_19-01-27/checkpoint_000002)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# train_llm_ray_lightning.py\n",
    "import torch\n",
    "import pytorch_lightning as pl                  # new import style in Lightning ≥2.0\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.data.data_manager import LightningDataModule\n",
    "\n",
    "# --- Ray Train imports -------------------------------------------------------\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "from ray.train import ScalingConfig, RunConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer        # replaces RayLightningTrainer\n",
    "from ray.train.lightning import (               # V2 Lightning helpers\n",
    "    RayDeepSpeedStrategy,                       # Ray-compatible ZeRO-3\n",
    "    RayLightningEnvironment,                    # cluster-aware env plugin\n",
    "    RayTrainReportCallback,                     # metrics/ckpt reporter\n",
    "    prepare_trainer,                            # patches the PL trainer\n",
    ")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import ray\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class LLMModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, learning_rate=1e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #default is crossentropy\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "        out = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids,\n",
    "        )\n",
    "        return out.loss\n",
    "\n",
    "    def training_step(self, batch,  _):\n",
    "        loss = self(**batch)\n",
    "        self.log(\"loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # with Deepspeed You are using ZeRO-Offload with a client provided\n",
    "        #  optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance.\n",
    "        #  Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config \n",
    "        # return torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    \"\"\"Executed once per Ray worker.\"\"\"\n",
    "    model_name = config.get(\"model_name\", \"gpt2\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    datamodule = LightningDataModule(\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_configs={\"wikitext\": {}},\n",
    "        batch_size=2,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    model = LLMModel(model_name=model_name)\n",
    "\n",
    "    ds_cfg_gpu = {\n",
    "        # ---- batching ----------------------------------------------------------\n",
    "        \"train_micro_batch_size_per_gpu\": 4,      # fits comfortably on an A100\n",
    "         #\"gradient_accumulation_steps\": 2,         # 4 × 2 × 8 GPUs  = 64/global step\n",
    "        \"gradient_clipping\": 1.0,\n",
    "\n",
    "        # ---- numerics ----------------------------------------------------------\n",
    "        \"fp16\": { \"enabled\": True },              # or switch to \"bf16\":{…}\n",
    "\n",
    "        # ---- ZeRO ----------------------------------------------------------------\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\":            500_000_000,\n",
    "            \"stage3_prefetch_bucket_size\":   500_000_000,\n",
    "            \"stage3_param_persistence_threshold\": 1_000_000\n",
    "        },\n",
    "\n",
    "        # ---- optimizer created by DeepSpeed (GPU AdamW fused) -------------------\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": 1e-5,\n",
    "                \"betas\": [0.9, 0.999],\n",
    "                \"eps\": 1e-8,\n",
    "                \"weight_decay\": 0.01\n",
    "            }\n",
    "        },\n",
    "\n",
    "        # ---- optional LR scheduler ---------------------------------------------\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"WarmupDecayLR\",\n",
    "            \"params\": {\n",
    "                \"total_num_steps\": 10_000,\n",
    "                \"warmup_num_steps\": 1_000,\n",
    "                \"warmup_min_lr\": 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Ray-aware DeepSpeed ZeRO-3 strategy\n",
    "    strategy = RayDeepSpeedStrategy(stage=3, config=ds_cfg_gpu)\n",
    "    \n",
    "    tb_logger = TensorBoardLogger(save_dir=\"tb_logs\", name=\"run1\")\n",
    "\n",
    "    # Standard Lightning trainer plus Ray plugins/callback\n",
    "    pl_trainer = pl.Trainer(\n",
    "        strategy=strategy,\n",
    "        accelerator=\"auto\",           # Ray sets CUDA_VISIBLE_DEVICES per worker\n",
    "        devices=\"auto\",\n",
    "        precision=\"16-mixed\",\n",
    "        max_epochs=3,\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        log_every_n_steps=5,\n",
    "        logger= tb_logger\n",
    "    )\n",
    "\n",
    "    # Patch the trainer for Ray & validate the config\n",
    "    pl_trainer = prepare_trainer(pl_trainer)\n",
    "\n",
    "    # Launch training\n",
    "    pl_trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def init_ray():\n",
    "    if ray.is_initialized():\n",
    "        print(\"RAy already up and running\")\n",
    "        return\n",
    "    else:\n",
    "        ray.init(address=\"10.141.1.24:6379\")\n",
    "        if ray.is_initialized():\n",
    "            print(\"RAy already up and running\")\n",
    "        else: \n",
    "            raise Exception\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    init_ray()\n",
    "    scaling = ScalingConfig(\n",
    "        num_workers=8,               # 8 GPUs total\n",
    "        use_gpu=True,\n",
    "        resources_per_worker={\"CPU\": 2, \"GPU\": 1},\n",
    "        placement_strategy=\"SPREAD\"\n",
    "    )\n",
    "\n",
    "    trainer = TorchTrainer(          # <- NEW\n",
    "        train_loop_per_worker=train_loop_per_worker,\n",
    "        scaling_config=scaling,\n",
    "        run_config=RunConfig(\n",
    "            checkpoint_config=CheckpointConfig(\n",
    "                num_to_keep=3,\n",
    "                checkpoint_score_attribute=\"val_loss\",\n",
    "                checkpoint_score_order=\"min\",\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
